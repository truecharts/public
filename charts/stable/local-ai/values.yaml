image:
  repository: docker.io/localai/localai
  pullPolicy: IfNotPresent
  tag: v2.22.1@sha256:d115a657fd5938ba3185d9c947babc0b5c9da8c272469091cf9c7171bb3e6f28
ffmpegImage:
  repository: docker.io/localai/localai
  pullPolicy: IfNotPresent
  tag: v2.22.1-ffmpeg-core@sha256:4c95ed59701b7365abe291f1b2f0b0e53d912b39379fefba8282b361f8c1a742
cublasCuda12Image:
  repository: docker.io/localai/localai
  pullPolicy: IfNotPresent
  tag: v2.22.1-cublas-cuda12-core@sha256:b02bf45acc1c234f8b079d6dffba12d9e1c20cfd83154f617ee09fe9c94366b3
cublasCuda12FfmpegImage:
  repository: docker.io/localai/localai
  pullPolicy: IfNotPresent
  tag: v2.22.1-cublas-cuda12-ffmpeg-core@sha256:5d941c90da03cba131daf9f9a4c1ae6133b3686ddf79973c9fb7e5f40448c65b
cublasCuda11Image:
  repository: docker.io/localai/localai
  pullPolicy: IfNotPresent
  tag: v2.22.1-cublas-cuda11-core@sha256:c392dab07d2672b4c0b5da02c0f7063b8ab42f11cd43508f21acf4aeb6c93660
cublasCuda11FfmpegImage:
  repository: docker.io/localai/localai
  pullPolicy: IfNotPresent
  tag: v2.22.1-cublas-cuda11-ffmpeg-core@sha256:5e6682ce53db6a6f271aa0d400f0801007df94e98d1e40c1641dc6366ead33b4
allInOneCuda12Image:
  repository: docker.io/localai/localai
  pullPolicy: IfNotPresent
  tag: v2.22.1-aio-gpu-nvidia-cuda-12@sha256:577d5b84704bcce9201b4da5e08b2ee84c1ad145522216c8dcdcdcfbbd725a85
allInOneCuda11Image:
  repository: docker.io/localai/localai
  pullPolicy: IfNotPresent
  tag: v2.22.1-aio-gpu-nvidia-cuda-11@sha256:7302225fe722b3bfedb027baa7edbb4ad098d0952b62e260f1d46b6c2f0f485f
allInOneCpuImage:
  repository: docker.io/localai/localai
  pullPolicy: IfNotPresent
  tag: v2.22.1-aio-cpu@sha256:ada88782e0de87383223e3fa8c7bbe2885ed0c285f4698eb1437780a2eace855
securityContext:
  container:
    runAsNonRoot: false
    readOnlyRootFilesystem: false
    runAsUser: 0
    runAsGroup: 0
service:
  main:
    ports:
      main:
        protocol: http
        port: 8080
localai:
  # Specify a build type. Available: cublas, openblas, clblas.
  build_type: "openblas"
  debug: false
  cors: true
  cors_allow_origins: "*"
  galleries: []
  #  - name: model-gallery
  #    url: github:go-skynet/model-gallery/index.yaml
  preload_models: []
  #    url: github:go-skynet/model-gallery/gpt4all-j.yaml
  # UPLOAD_LIMIT
workload:
  main:
    podSpec:
      containers:
        main:
          probes:
            liveness:
              enabled: true
              type: http
              path: /readyz
            readiness:
              enabled: true
              type: http
              path: /readyz
            startup:
              enabled: true
              type: tcp
          imageSelector: image
          env:
            ADDRESS: ":{{ .Values.service.main.ports.main.port }}"
            MODELS_PATH: "{{ .Values.persistence.models.mountPath }}"
            IMAGE_PATH: "{{ .Values.persistence.images.mountPath }}"
            BUILD_TYPE: "{{ .Values.localai.build_type }}"
            # breaks chart if true, keep it false.
            REBUILD: false
            DEBUG: "{{ .Values.localai.debug }}"
            CORS: "{{ .Values.localai.cors }}"
            GALLERIES: "{{ toJson .Values.localai.galleries }}"
            PRELOAD_MODELS: "{{ toJson .Values.localai.preload_models }}"
            CORS_ALLOW_ORIGINS: "{{ .Values.localai.cors_allow_origins }}"
persistence:
  models:
    enabled: true
    mountPath: "/models"
  images:
    enabled: true
    mountPath: "/images"
portal:
  open:
    enabled: false
updated: true
